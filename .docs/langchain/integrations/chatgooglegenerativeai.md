# ChatGoogleGenerativeAI

> Get started using Gemini [chat models](/oss/python/langchain/models) in LangChain.

Access Google's Generative AI models, including the Gemini family, directly via the Gemini API or experiment rapidly using Google AI Studio. This is often the best starting point for individual developers.

For information on the latest models, model IDs, their features, context windows, etc. head to the [Google AI docs](https://ai.google.dev/gemini-api/docs).

<Tip>
  **API Reference**

  For detailed documentation of all features and configuration options, head to the [`ChatGoogleGenerativeAI`](https://reference.langchain.com/python/integrations/langchain_google_genai/#langchain_google_genai.ChatGoogleGenerativeAI) API reference.
</Tip>

## Overview

### Integration details

| Class                                                                                                                                                 | Package                                                                                                | Local | Serializable | [JS support](https://js.langchain.com/docs/integrations/chat/google_generative_ai) |                                                Downloads                                                |                                                Version                                               |
| :---------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------- | :---: | :----------: | :--------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------: |
| [`ChatGoogleGenerativeAI`](https://reference.langchain.com/python/integrations/langchain_google_genai/#langchain_google_genai.ChatGoogleGenerativeAI) | [`langchain-google-genai`](https://reference.langchain.com/python/integrations/langchain_google_genai) |   ❌   |     beta     |                                          ✅                                         | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-google-genai?style=flat-square\&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-google-genai?style=flat-square\&label=%20) |

### Model features

| [Tool calling](/oss/python/langchain/tools) | [Structured output](/oss/python/langchain/structured-output) | JSON mode | [Image input](/oss/python/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/python/langchain/streaming/) | Native async | [Token usage](/oss/python/langchain/models#token-usage) | [Logprobs](/oss/python/langchain/models#log-probabilities) |
| :-----------------------------------------: | :----------------------------------------------------------: | :-------: | :------------------------------------------------------: | :---------: | :---------: | :-------------------------------------------------------: | :----------: | :-----------------------------------------------------: | :--------------------------------------------------------: |
|                      ✅                      |                               ✅                              |     ✅     |                             ✅                            |      ✅      |      ✅      |                             ✅                             |       ✅      |                            ✅                            |                              ❌                             |

## Setup

To access Google AI models you'll need to create a Google Account, get a Google AI API key, and install the `langchain-google-genai` integration package.

### Installation

```python  theme={null}
pip install -U langchain-google-genai
```

### Credentials

Head to the [Google AI Studio](https://ai.google.dev/gemini-api/docs/api-key) to generate a Google AI API key. Once you've done this set the `GOOGLE_API_KEY` environment variable in your environment:

```python  theme={null}
import getpass
import os

if "GOOGLE_API_KEY" not in os.environ:
    os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter your Google AI API key: ")
```

To enable automated tracing of your model calls, set your [LangSmith](https://docs.smith.langchain.com/) API key:

```python  theme={null}
os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
os.environ["LANGSMITH_TRACING"] = "true"
```

## Instantiation

Now we can instantiate our model object and generate responses:

<CodeGroup>
  ```python Gemini 3 theme={null}
  from langchain_google_genai import ChatGoogleGenerativeAI

  model = ChatGoogleGenerativeAI(
      model="gemini-3-pro-preview",
      temperature=0,
      max_tokens=None,
      timeout=None,
      max_retries=2,
      # other params...
  )
  ```

  ```python Gemini 2.5 and earlier theme={null}
  from langchain_google_genai import ChatGoogleGenerativeAI

  model = ChatGoogleGenerativeAI(
      model="gemini-2.5-pro",
      temperature=0,
      max_tokens=None,
      timeout=None,
      max_retries=2,
      # other params...
  )
  ```
</CodeGroup>

See the [`ChatGoogleGenerativeAI`](https://reference.langchain.com/python/integrations/langchain_google_genai/#langchain_google_genai.ChatGoogleGenerativeAI) API Reference for the full set of available model parameters.

## Invocation

```python  theme={null}
messages = [
    (
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ),
    ("human", "I love programming."),
]
ai_msg = model.invoke(messages)
ai_msg
```

<CodeGroup>
  ```output Gemini 3 theme={null}
  AIMessage(content=[{'type': 'text', 'text': "J'adore la programmation.", 'extras': {'signature': 'EpoWCpc...'}}], additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-3-pro-preview', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--fb732b64-1ab4-4a28-b93b-dcfb2a164a3d-0', usage_metadata={'input_tokens': 21, 'output_tokens': 779, 'total_tokens': 800, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 772}})
  ```

  ```output Gemini 2.5 theme={null}
  AIMessage(content="J'adore la programmation.", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run-3b28d4b8-8a62-4e6c-ad4e-b53e6e825749-0', usage_metadata={'input_tokens': 20, 'output_tokens': 7, 'total_tokens': 27, 'input_token_details': {'cache_read': 0}})
  ```
</CodeGroup>

```python  theme={null}
print(ai_msg.content)
```

<CodeGroup>
  ```output Gemini 3 theme={null}
  [{'type': 'text',
  'text': "J'adore la programmation.",
  'extras': {'signature': '...'}}]
  ```

  ```output Gemini 2.5 theme={null}
  J'adore la programmation.
  ```
</CodeGroup>

<Note>
  **Message content shape**

  Gemini 3 series models will always return a list of content blocks to capture [thought signatures](#thought-signatures). Use the `.text` property to recover string content.

  ```python  theme={null}
  response.content  # -> [{"type": "text", "text": "Hello!", "extras": {"signature": "EpQFCp...lKx64r"}}]
  response.text     # -> "Hello!"
  ```
</Note>

## Multimodal usage

Gemini models can accept multimodal inputs (text, images, audio, video) and, for some models, generate multimodal outputs.

### Supported input methods

| Method                                  | [Image](#image-input) | [Video](#video-input) | [Audio](#audio-input) | [PDF](#pdf-input) |
| --------------------------------------- | :-------------------: | :-------------------: | :-------------------: | :---------------: |
| [File upload](#file-upload) (Files API) |           ✅           |           ✅           |           ✅           |         ✅         |
| Base64 inline data                      |           ✅           |           ✅           |           ✅           |         ✅         |
| HTTP/HTTPS URLs                         |           ✅           |           ❌           |           ❌           |         ❌         |
| YouTube URLs                            |           ❌           |           ✅           |           ❌           |         ❌         |
| GCS URIs (`gs://...`)                   |           ✅           |           ✅           |           ✅           |         ✅         |

### Image input

Provide image inputs along with text using a [`HumanMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.HumanMessage) with a list content format.

<CodeGroup>
  ```python Image URL theme={null}
  from langchain.messages import HumanMessage
  from langchain_google_genai import ChatGoogleGenerativeAI

  model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")

  message = HumanMessage(
      content=[
          {"type": "text", "text": "Describe the image at the URL."},
          {
              "type": "image",
              "url": "https://picsum.photos/seed/picsum/200/300",
          },
      ]
  )
  response = model.invoke([message])
  ```

  ```python Base64 encoded theme={null}
  import base64
  from langchain.messages import HumanMessage
  from langchain_google_genai import ChatGoogleGenerativeAI

  model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")

  image_bytes = open("path/to/your/image.jpg", "rb").read()
  image_base64 = base64.b64encode(image_bytes).decode("utf-8")
  mime_type = "image/jpeg"

  message = HumanMessage(
      content=[
          {"type": "text", "text": "Describe the local image."},
          {
              "type": "image",
              "base64": image_base64,
              "mime_type": mime_type,
          },
      ]
  )
  response = model.invoke([message])
  ```

  ```python Chat Completions image_url format theme={null}
  from langchain.messages import HumanMessage
  from langchain_google_genai import ChatGoogleGenerativeAI

  model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")

  message = HumanMessage(
      content=[
          {"type": "text", "text": "Describe the image at the URL."},
          {"type": "image_url", "image_url": "https://picsum.photos/seed/picsum/200/300"},
      ]
  )
  response = model.invoke([message])
  ```
</CodeGroup>

Other supported image formats:

* A Google Cloud Storage URI (`gs://...`). Ensure the service account has access.

### PDF input

Provide PDF file inputs along with text.

```python  theme={null}
import base64
from langchain.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI

model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")

pdf_bytes = open("path/to/your/document.pdf", "rb").read()
pdf_base64 = base64.b64encode(pdf_bytes).decode("utf-8")
mime_type = "application/pdf"

message = HumanMessage(
    content=[
        {"type": "text", "text": "Describe the document in a sentence."},
        {
            "type": "file",
            "base64": pdf_base64,
            "mime_type": mime_type,
        },
    ]
)
response = model.invoke([message])
```

### Audio input

Provide audio file inputs along with text.

```python  theme={null}
import base64
from langchain.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI

model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")

audio_bytes = open("path/to/your/audio.mp3", "rb").read()
audio_base64 = base64.b64encode(audio_bytes).decode("utf-8")
mime_type = "audio/mpeg"

message = HumanMessage(
    content=[
        {"type": "text", "text": "Summarize this audio in a sentence."},
        {
            "type": "audio",
            "base64": audio_base64,
            "mime_type": mime_type,
        },
    ]
)
response = model.invoke([message])
```

### Video input

Provide video file inputs along with text.

<CodeGroup>
  ```python Base64 encoded theme={null}
  import base64
  from langchain.messages import HumanMessage
  from langchain_google_genai import ChatGoogleGenerativeAI

  model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")

  video_bytes = open("path/to/your/video.mp4", "rb").read()
  video_base64 = base64.b64encode(video_bytes).decode("utf-8")
  mime_type = "video/mp4"

  message = HumanMessage(
      content=[
          {"type": "text", "text": "Describe what's in this video in a sentence."},
          {
              "type": "video",
              "base64": video_base64,
              "mime_type": mime_type,
          },
      ]
  )
  response = model.invoke([message])
  print(response.text)
  ```
</CodeGroup>

<Note>
  **YouTube limitations**

  * Only public videos are supported (not private or unlisted)
  * Free tier: max 8 hours of YouTube video per day
  * Feature is currently in preview
</Note>

### File upload

You can upload files to Google's servers and reference them by URI. This works for PDFs, images, videos, and audio files.

```python  theme={null}
import time
from google import genai
from langchain.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI

client = genai.Client()
model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")

# Upload file to Google's servers
myfile = client.files.upload(file="/path/to/your/sample.pdf")
while myfile.state.name == "PROCESSING":
    time.sleep(2)
    myfile = client.files.get(name=myfile.name)

# Reference by URI
message = HumanMessage(
    content=[
        {"type": "text", "text": "What is in the document?"},
        {
            "type": "media",
            "file_uri": myfile.uri,
            "mime_type": "application/pdf",
        },
    ]
)
response = model.invoke([message])
```

### Image generation

Certain models (such as `gemini-2.5-flash-image`) can generate text and images inline.

See more information on the [Gemini API docs](https://ai.google.dev/gemini-api/docs/image-generation) for details.

```python  theme={null}
import base64

from IPython.display import Image, display
from langchain.messages import AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI, Modality

model = ChatGoogleGenerativeAI(model="gemini-2.5-flash-image")

message = {
    "role": "user",
    "content": "Generate a photorealistic image of a cuddly cat wearing a hat.",
}

model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")

response = model.invoke(
    [message],
    response_modalities=[Modality.TEXT, Modality.IMAGE],
)


def _get_image_base64(response: AIMessage) -> None:
    image_block = next(
        block
        for block in response.content
        if isinstance(block, dict) and block.get("image_url")
    )
    return image_block["image_url"].get("url").split(",")[-1]


image_base64 = _get_image_base64(response)
display(Image(data=base64.b64decode(image_base64), width=300))
```

### Audio generation

Certain models (such as `gemini-2.5-flash-preview-tts`) can generate audio files.

See more information on the [Gemini API docs](https://ai.google.dev/gemini-api/docs/speech-generation) for details.

```python  theme={null}
from langchain_google_genai import ChatGoogleGenerativeAI

model = ChatGoogleGenerativeAI(model="gemini-2.5-flash-preview-tts")

response = model.invoke(
    "Please say The quick brown fox jumps over the lazy dog",
    generation_config=dict(response_modalities=["AUDIO"]),
)

# Base64 encoded binary data of the audio
wav_data = response.additional_kwargs.get("audio")
with open("output.wav", "wb") as f:
    f.write(wav_data)
```

## Tool calling

You can equip the model with tools to call.

```python  theme={null}
from langchain.tools import tool
from langchain.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI


# Define the tool
@tool(description="Get the current weather in a given location")
def get_weather(location: str) -> str:
    return "It's sunny."


# Initialize and bind (potentially multiple) tools to the model
model_with_tools = ChatGoogleGenerativeAI(model="gemini-3-pro-preview").bind_tools([get_weather])

# Step 1: Model generates tool calls
messages = [HumanMessage("What's the weather in Boston?")]
ai_msg = model_with_tools.invoke(messages)
messages.append(ai_msg)

# Check the tool calls in the response
print(ai_msg.tool_calls)

# Step 2: Execute tools and collect results
for tool_call in ai_msg.tool_calls:
    # Execute the tool with the generated arguments
    tool_result = get_weather.invoke(tool_call)
    messages.append(tool_result)

# Step 3: Pass results back to model for final response
final_response = model_with_tools.invoke(messages)
final_response
```

```output  theme={null}
[{'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': '879b4233-901b-4bbb-af56-3771ca8d3a75', 'type': 'tool_call'}]
```

```output  theme={null}
AIMessage(content=[{'type': 'text', 'text': 'The weather in Boston is sunny.'}], additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-3-pro-preview', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--190be543-c974-460b-a708-7257892c3121-0', usage_metadata={'input_tokens': 143, 'output_tokens': 7, 'total_tokens': 150, 'input_token_details': {'cache_read': 0}})
```

## Structured output

Force the model to respond with a specific structure. See the [Gemini API docs](https://ai.google.dev/gemini-api/docs/structured-output) for more info.

```python  theme={null}
from langchain_google_genai import ChatGoogleGenerativeAI
from pydantic import BaseModel
from typing import Literal


class Feedback(BaseModel):
    sentiment: Literal["positive", "neutral", "negative"]
    summary: str


model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")
structured_model = model.with_structured_output(
    schema=Feedback.model_json_schema(), method="json_schema"
)

response = structured_model.invoke("The new UI is great!")
response["sentiment"]  # "positive"
response["summary"]  # "The user expresses positive..."
```

For streaming structured output, merge dictionaries instead of using `+=`:

```python  theme={null}
stream = structured_model.stream("The interface is intuitive and beautiful!")
full = next(stream)
for chunk in stream:
    full.update(chunk)  # Merge dictionaries
print(full)  # Complete structured response
# -> {'sentiment': 'positive', 'summary': 'The user praises...'}
```

### Structured output methods

Two methods are supported for structured output:

* **`method="function_calling"` (default)**: Uses tool calling to extract structured data.
* **`method="json_schema"`**: Uses Gemini's native structured output.

The `json_schema` method is **recommended for better reliability** as it constrains the model's generation process directly rather than relying on post-processing tool calls.

## Token usage tracking

Access token usage information from the response metadata.

```python  theme={null}
from langchain_google_genai import ChatGoogleGenerativeAI

model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")

result = model.invoke("Explain the concept of prompt engineering in one sentence.")

print(result.content)
print("\nUsage Metadata:")
print(result.usage_metadata)
```

```output  theme={null}
Prompt engineering is the art and science of crafting effective text prompts to elicit desired and accurate responses from large language models.

Usage Metadata:
{'input_tokens': 10, 'output_tokens': 24, 'total_tokens': 34, 'input_token_details': {'cache_read': 0}}
```

## Thinking support

With reasoning models, you have the option to adjust the number of internal thinking tokens used (thinking\_budget) or to disable thinking altogether.

<Warning>
  Not all models allow disabling thinking. See the [Gemini models documentation](https://ai.google.dev/gemini-api/docs/models) for details.
</Warning>

To see a thinking model's thoughts, set `include_thoughts=True` to have the model's reasoning summaries included in the response.

```python  theme={null}
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="gemini-3-pro-preview",
    thinking_budget=1024, # [!code highlight]
    include_thoughts=True, # [!code highlight]
)

response = llm.invoke("How many O's are in Google? How did you verify your answer?")
reasoning_score = response.usage_metadata["output_token_details"]["reasoning"]

print("Response:", response.content)
print("Reasoning tokens used:", reasoning_score)
```

```output  theme={null}
Response: [{'type': 'thinking', 'thinking': '**Analyzing and Cou...'}, {'type': 'text', 'text': 'There a...', 'extras': {'signature': 'EroR...'}}]
Reasoning tokens used: 672
```

See the [Gemini API docs](https://ai.google.dev/gemini-api/docs/thinking) for more information on thinking.

### Thought signatures

[Thought signatures](https://ai.google.dev/gemini-api/docs/thought-signatures) are encrypted representations of the model's reasoning processes.

<Note>
  Gemini 3 may raise 4xx errors if thought signatures are not passed back with tool call responses. Upgrade to `langchain-google-genai >= 3.1.0` to ensure this is handled correctly.
</Note>

```python  theme={null}
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="gemini-3-pro-preview",
    thinking_budget=1024,
    include_thoughts=True,
)

response = llm.invoke("How many O's are in Google? How did you verify your answer?")

response.content_blocks[-1]
# -> {"type": "text", "text": "...", "extras": {"signature": "EtgVCt..."}}
```

## Built-in tools

Google Gemini supports a variety of built-in tools, which can be bound to the model in the usual way.

### Google search

See [Gemini docs](https://ai.google.dev/gemini-api/docs/grounding/search-suggestions) for detail.

```python  theme={null}
from langchain_google_genai import ChatGoogleGenerativeAI

model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")

model_with_search = model.bind_tools([{"google_search": {}}])
response = model_with_search.invoke("When is the next total solar eclipse in US?")

response.content_blocks
```

```output  theme={null}
[{'type': 'text',
  'text': 'The next total solar eclipse visible in the contiguous United States will occur on...',
  'annotations': [{'type': 'citation',
    'id': 'abc123',
    'url': '<url for source 1>',
    'title': '<source 1 title>',
    'start_index': 0,
    'end_index': 99,
    'cited_text': 'The next total solar eclipse...',
    'extras': {'google_ai_metadata': {'web_search_queries': ['next total solar eclipse in US'],
       'grounding_chunk_index': 0,
       'confidence_scores': []}}},
   {'type': 'citation',
    'id': 'abc234',
    'url': '<url for source 2>',
    'title': '<source 2 title>',
    'start_index': 0,
    'end_index': 99,
    'cited_text': 'The next total solar eclipse...',
    'extras': {'google_ai_metadata': {'web_search_queries': ['next total solar eclipse in US'],
       'grounding_chunk_index': 1,
       'confidence_scores': []}}}]}]
```

### Code execution

See [Gemini docs](https://ai.google.dev/gemini-api/docs/code-execution?lang=python) for detail.

```python  theme={null}
from langchain_google_genai import ChatGoogleGenerativeAI

model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")

model_with_code_interpreter = model.bind_tools([{"code_execution": {}}])
response = model_with_code_interpreter.invoke("Use Python to calculate 3^3.")

response.content_blocks
```

```output  theme={null}
[{'type': 'server_tool_call',
  'name': 'code_interpreter',
  'args': {'code': 'print(3**3)', 'language': <Language.PYTHON: 1>},
  'id': '...'},
 {'type': 'server_tool_result',
  'tool_call_id': '',
  'status': 'success',
  'output': '27\n',
  'extras': {'block_type': 'code_execution_result',
   'outcome': <Outcome.OUTCOME_OK: 1>}},
 {'type': 'text', 'text': 'The calculation of 3 to the power of 3 is 27.'}]
```

## Safety settings

Gemini models have default safety settings that can be overridden. If you are receiving lots of `'Safety Warnings'` from your models, you can try tweaking the `safety_settings` attribute of the model. For example, to turn off safety blocking for dangerous content, you can construct your LLM as follows:

```python  theme={null}
from langchain_google_genai import (
    ChatGoogleGenerativeAI,
    HarmBlockThreshold,
    HarmCategory,
)

llm = ChatGoogleGenerativeAI(
        model="gemini-3-pro-preview",
        safety_settings={
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    },
)
```

For an enumeration of the categories and thresholds available, see Google's [safety setting types](https://ai.google.dev/api/python/google/generativeai/types/SafetySettingDict).

## Context caching

Context caching allows you to store and reuse content (e.g., PDFs, images) for faster processing. The `cached_content` parameter accepts a cache name created via the Google Generative AI API.

<Accordion title="Single file example">
  This caches a single file and queries it.

  ```python  theme={null}
  import time
  from google import genai
  from google.genai import types
  from langchain.messages import HumanMessage
  from langchain_google_genai import ChatGoogleGenerativeAI

  client = genai.Client()

  # Upload file
  file = client.files.upload(file="path/to/your/file")
  while file.state.name == "PROCESSING":
      time.sleep(2)
      file = client.files.get(name=file.name)

  # Create cache
  model = "gemini-3-pro-preview"
  cache = client.caches.create(
      model=model,
      config=types.CreateCachedContentConfig(
          display_name="Cached Content",
          system_instruction=(
              "You are an expert content analyzer, and your job is to answer "
              "the user's query based on the file you have access to."
          ),
          contents=[file],
          ttl="300s",
      ),
  )

  # Query with LangChain
  llm = ChatGoogleGenerativeAI(
      model=model,
      cached_content=cache.name,
  )
  message = HumanMessage(content="Summarize the main points of the content.")
  llm.invoke([message])
  ```
</Accordion>

<Accordion title="Multiple files example">
  This caches two files using `Part` and queries them together.

  ```python  theme={null}
  import time
  from google import genai
  from google.genai.types import CreateCachedContentConfig, Content, Part
  from langchain.messages import HumanMessage
  from langchain_google_genai import ChatGoogleGenerativeAI

  client = genai.Client()

  # Upload files
  file_1 = client.files.upload(file="./file1")
  while file_1.state.name == "PROCESSING":
      time.sleep(2)
      file_1 = client.files.get(name=file_1.name)

  file_2 = client.files.upload(file="./file2")
  while file_2.state.name == "PROCESSING":
      time.sleep(2)
      file_2 = client.files.get(name=file_2.name)

  # Create cache with multiple files
  contents = [
      Content(
          role="user",
          parts=[
              Part.from_uri(file_uri=file_1.uri, mime_type=file_1.mime_type),
              Part.from_uri(file_uri=file_2.uri, mime_type=file_2.mime_type),
          ],
      )
  ]
  model = "gemini-3-pro-preview"
  cache = client.caches.create(
      model=model,
      config=CreateCachedContentConfig(
          display_name="Cached Contents",
          system_instruction=(
              "You are an expert content analyzer, and your job is to answer "
              "the user's query based on the files you have access to."
          ),
          contents=contents,
          ttl="300s",
      ),
  )

  # Query with LangChain
  llm = ChatGoogleGenerativeAI(
      model=model,
      cached_content=cache.name,
  )
  message = HumanMessage(
      content="Provide a summary of the key information across both files."
  )
  llm.invoke([message])
  ```
</Accordion>

See the Gemini API docs on [context caching](https://ai.google.dev/gemini-api/docs/caching?lang=python) for more information.

## Response metadata

Access response metadata from the model response.

```python  theme={null}
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")

response = llm.invoke("Hello!")
response.response_metadata
```

```output  theme={null}
{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []},
 'finish_reason': 'STOP',
 'model_name': 'gemini-3-pro-preview',
 'safety_ratings': [],
 'model_provider': 'google_genai'}
```

***

## API reference

For detailed documentation of all features and configuration options, head to the [`ChatGoogleGenerativeAI`](https://reference.langchain.com/python/integrations/langchain_google_genai/#langchain_google_genai.ChatGoogleGenerativeAI) API reference.

***

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/chat/google_generative_ai.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>


---

> To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt